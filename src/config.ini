[model_config]
# location of train set
train=/usr/lvch/GRAM-CNN/dataset/ncbi/train.eng

# location of dev set
dev=/usr/lvch/GRAM-CNN/dataset/ncbi/dev.eng

# location of test set
test=/usr/lvch/GRAM-CNN/dataset/ncbi/test.eng

# location of pre-train embedding
pre_emb=/usr/lvch/GRAM-CNN/embeddings/bio_nlp_vec/PubMed-shuffle-win-30.bin

# location of pre_train bioBert embedding
bio_bert_embedding=/home/wds/下载/biobert_v1.1_pubmed/model.ckpt-1000000

# location of pre-train bioBert vocab
bio_bert_vocab=/home/wds/下载/biobert_v1.1_pubmed/vocab.txt

# reload model path`
model_path=use_wordTrue use_charTrue drop_out0.5 hidden_size100 hidden_layer1 lowerTrue allembTrue kernels2, 3, 4 num_kernels40, 40, 40 paddingFalse ptsFalse w_emb2002021-01-14 15:11:04.529419
# lstm hidden layer size
word_lstm_dim=100

hidden_layer=1

# dropout rate
dropout=0.5

# lowercase the words
lower=0

# load all embedding
all_emb=1

# tag scheme
tag_scheme=iob

# capitalization feature dimension
cap_dim=0

# use crf
crf=1

# whether padding the input to use gram-CNN
padding=0

# whether use the pts tagging
pts=0

# replace digits with 0
zeros=0

# char embedding dim
char_dim=25

# char lstm hidden layer size
char_lstm_dim=25

# use a bidirectional LSTM for chars
char_bidirect=1

# use a bidirectional LSTM for words
word_bidirect=1

# word embedding dim
word_dim=200

# bio_bert_dim
bio_bert_dim = 768
# lr method
lr_method=sgd-lr_.005

# reload last model
reload=0

# whether to use word embedding
use_word=1


# whether to use bioBert word embedding
use_bert_word=1
# whether to use char embedding
use_char=1

# CNN kernel size
kernels=2,3,4

# kernel num
kernel_num=100,100,100

# epochs
epochs=150

# freq_eval
freq_eval=1000
