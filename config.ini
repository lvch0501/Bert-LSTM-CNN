[model_config]
# location of train set
train=/Users/lch/PycharmProjects/GRAM-CNN/dataset/ncbi/train.eng

# location of dev set
dev=/Users/lch/PycharmProjects/GRAM-CNN/datasett/ncbi/dev.eng

# location of test set
test=/Users/lch/PycharmProjects/GRAM-CNN/dataset/ncbi/test.eng

# location of pre-train embedding
pre_emb=/Users/lch/PycharmProjects/GRAM-CNN/embeddings/bio_nlp_vec/PubMed-shuffle-win-30.bin

# lstm hidden layer size
word_lstm_dim=100

hidden_layer=1

# dropout rate
dropout=0.5

# lowercase the words
lower=1

# load all embedding
all_emb=1

# tag scheme
tag_scheme=iob

# capitalization feature dimension
cap_dim=0

# use crf
crf=1

# whether padding the input to use gram-CNN
padding=0

# whether use the pts tagging
pts=0

# replace digits with 0
zeros=0

# char embedding dim
char_dim=25

# char lstm hidden layer size
char_lstm_dim=25

# use a bidirectional LSTM for chars
char_bidirect=1

# use a bidirectional LSTM for words
word_bidirect=1

# word embedding dim
word_dim=200

# lr method
lr_method=sgd-lr_.005

# reload last model
reload=0

# whether to user word embedding
use_word=1

# whether to user char embedding
use_char=1

# CNN kernel size
kernels=2,3,4

# kernel num
kernel_num=40,40,40